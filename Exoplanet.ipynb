{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa-title",
   "metadata": {},
   "source": [
    "# Machine Learning-Based Classification of Exoplanets Using Orbital and Physical Parameters\n",
    "\n",
    "---\n",
    "\n",
    "**Abstract** — The detection and classification of exoplanets represents a fundamental\n",
    "challenge in modern astrophysics. This paper presents a comparative study of machine\n",
    "learning approaches for classifying exoplanets by discovery method using four key\n",
    "physical and orbital parameters: orbital period, planetary mass, equilibrium temperature,\n",
    "and insolation flux. Three classifiers are evaluated — Random Forest, XGBoost, and a\n",
    "Deep Neural Network — trained on the NASA Exoplanet Archive 2025 dataset comprising\n",
    "38,090 records across 11 discovery-method classes. The XGBoost model achieves the\n",
    "highest overall accuracy of 95.3%. Feature-importance analysis identifies insolation\n",
    "flux and equilibrium temperature as the most discriminative features. These findings\n",
    "demonstrate the viability of machine learning for large-scale exoplanet classification\n",
    "and provide insights for future automated planetary-detection pipelines.\n",
    "\n",
    "**Index Terms** — Exoplanet classification, machine learning, Random Forest, XGBoost,\n",
    "deep learning, NASA Exoplanet Archive, orbital parameters, discovery method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab-intro",
   "metadata": {},
   "source": [
    "## I. Introduction\n",
    "\n",
    "The discovery of exoplanets — planets orbiting stars beyond our solar system — has\n",
    "accelerated dramatically since the first confirmed detection in 1992 [1]. Multiple\n",
    "detection techniques have been developed, including the Transit method, Radial Velocity\n",
    "(RV), Direct Imaging, Microlensing, and Pulsar Timing, each sensitive to different\n",
    "classes of planetary systems and stellar environments.\n",
    "\n",
    "As exoplanet catalogs grow to tens of thousands of entries, manual analysis becomes\n",
    "increasingly impractical. Machine learning (ML) offers a promising avenue for automated\n",
    "classification and characterization of exoplanets using their measured physical and\n",
    "orbital properties. Accurate classification of discovery methods is useful not only for\n",
    "catalog verification but also for understanding planetary demographics and observational\n",
    "selection biases.\n",
    "\n",
    "In this paper we investigate multi-class classification of exoplanets by discovery method\n",
    "using four orbital and physical parameters from the NASA Exoplanet Archive. We compare\n",
    "three ML classifiers — Random Forest (RF), XGBoost, and a Deep Neural Network (DNN) —\n",
    "and analyse feature importance to identify the most discriminative physical properties.\n",
    "\n",
    "The remainder of this paper is organized as follows: Section II reviews related work;\n",
    "Section III describes the dataset and preprocessing pipeline; Section IV presents\n",
    "exploratory data analysis; Section V details the methodology; Section VI presents and\n",
    "discusses results; and Section VII concludes the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac-related",
   "metadata": {},
   "source": [
    "## II. Related Work\n",
    "\n",
    "Several studies have applied ML to exoplanet detection and classification. Pearson\n",
    "et al. [2] employed deep learning for automated identification of transit signals in\n",
    "Kepler data, achieving high precision in distinguishing true transits from false positives.\n",
    "Shallue & Vanderburg [3] developed AstroNet, a convolutional neural network that\n",
    "classified Kepler Objects of Interest, demonstrating the potential of deep learning for\n",
    "transit-based exoplanet detection.\n",
    "\n",
    "Mislis et al. [4] applied support vector machines to classify exoplanets from\n",
    "photometric surveys, while Dattilo et al. [5] extended the AstroNet framework to K2\n",
    "mission data. Gradient-boosting methods have also been applied to exoplanet radius-gap\n",
    "classification [6], providing insight into the bimodal distribution of sub-Neptune and\n",
    "super-Earth populations.\n",
    "\n",
    "Unlike prior work focused on transit photometry, this study employs broader physical and\n",
    "orbital parameters to classify across *all* major detection methods simultaneously,\n",
    "providing a multi-class framework applicable to heterogeneous exoplanet catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.impute import KNNImputer\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plot style\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams.update({'figure.dpi': 100, 'font.size': 11})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca-dataset",
   "metadata": {},
   "source": [
    "## III. Dataset and Preprocessing\n",
    "\n",
    "### A. Data Source\n",
    "\n",
    "The dataset is obtained from the **NASA Exoplanet Archive** [7], accessed in 2025.\n",
    "It contains 38,090 records representing confirmed and published exoplanets across\n",
    "100 raw attributes including planetary orbital parameters, stellar properties,\n",
    "photometric measurements, and discovery metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = 'all_exoplanets_2025.csv'\n",
    "df_raw = pd.read_csv(data_path)\n",
    "print(f'Raw dataset shape: {df_raw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc-head",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd-feat-md",
   "metadata": {},
   "source": [
    "### B. Feature Selection\n",
    "\n",
    "From the 100 available attributes, 12 columns were selected based on physical\n",
    "relevance and prior literature. Table I summarises the retained features.\n",
    "\n",
    "**Table I: Selected Features and Descriptions**\n",
    "\n",
    "| Feature | Description | Unit |\n",
    "|---|---|---|\n",
    "| `orbital_period_days` | Time for one full orbit | Days |\n",
    "| `planet_radius_earth_radii` | Planetary radius | Earth radii |\n",
    "| `planet_mass_earth_masses` | Planetary mass | Earth masses |\n",
    "| `insolation_flux_earth_1` | Stellar flux received relative to Earth | Earth = 1 |\n",
    "| `equilibrium_temperature_kelvin` | Blackbody equilibrium temperature | Kelvin |\n",
    "| `stellar_temperature_kelvin` | Host star effective temperature | Kelvin |\n",
    "| `stellar_radius_solar_radii` | Host star radius | Solar radii |\n",
    "| `stellar_mass_solar_masses` | Host star mass | Solar masses |\n",
    "| `distance_to_system_parsecs` | Distance to the planetary system | Parsecs |\n",
    "| `discovery_method` | Method used to detect the planet *(target)* | — |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection and renaming\n",
    "selected_columns = {\n",
    "    'pl_name'       : 'planet_name',\n",
    "    'discoverymethod': 'discovery_method',\n",
    "    'disc_year'     : 'discovery_year',\n",
    "    'pl_orbper'     : 'orbital_period_days',\n",
    "    'pl_rade'       : 'planet_radius_earth_radii',\n",
    "    'pl_masse'      : 'planet_mass_earth_masses',\n",
    "    'pl_insol'      : 'insolation_flux_earth_1',\n",
    "    'pl_eqt'        : 'equilibrium_temperature_kelvin',\n",
    "    'st_teff'       : 'stellar_temperature_kelvin',\n",
    "    'st_rad'        : 'stellar_radius_solar_radii',\n",
    "    'st_mass'       : 'stellar_mass_solar_masses',\n",
    "    'sy_dist'       : 'distance_to_system_parsecs',\n",
    "}\n",
    "\n",
    "df = df_raw[list(selected_columns.keys())].rename(columns=selected_columns)\n",
    "print(f'Selected feature set shape: {df.shape}')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf-missing-md",
   "metadata": {},
   "source": [
    "### C. Missing Value Analysis\n",
    "\n",
    "Inspection of the selected columns reveals substantial missing values, particularly\n",
    "for `planet_mass_earth_masses` (89.3%), `insolation_flux_earth_1` (56.1%), and\n",
    "`equilibrium_temperature_kelvin` (56.2%). This is expected given the diversity of\n",
    "detection methods: transit-detected planets often lack mass measurements, while\n",
    "radial-velocity planets lack radius measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cg-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value summary\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing (%)': missing_pct})\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch-describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of key numerical columns\n",
    "df.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ci-impute-md",
   "metadata": {},
   "source": [
    "### D. Missing Value Imputation\n",
    "\n",
    "K-Nearest Neighbours (KNN) imputation with *k = 5* neighbours was applied to the\n",
    "four primary classification features to preserve sample size while minimising bias\n",
    "introduced by mean/median substitution [8]. After imputation all four model-input\n",
    "features are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cj-impute-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN imputation on the four model-input features\n",
    "impute_cols = [\n",
    "    'orbital_period_days',\n",
    "    'planet_mass_earth_masses',\n",
    "    'equilibrium_temperature_kelvin',\n",
    "    'insolation_flux_earth_1',\n",
    "]\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[impute_cols] = knn_imputer.fit_transform(df[impute_cols])\n",
    "\n",
    "print('Missing values after KNN imputation:')\n",
    "print(df[impute_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da-eda-md",
   "metadata": {},
   "source": [
    "## IV. Exploratory Data Analysis\n",
    "\n",
    "This section presents key statistical visualisations to identify distributional\n",
    "patterns, class imbalances, and inter-feature relationships within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db-pairplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 1 – Pairplot of primary classification features\n",
    "df_plot = df[impute_cols].rename(columns={\n",
    "    'orbital_period_days'           : 'Orbital Period (days)',\n",
    "    'planet_mass_earth_masses'       : 'Mass (Earth Masses)',\n",
    "    'equilibrium_temperature_kelvin' : 'Equil. Temp (K)',\n",
    "    'insolation_flux_earth_1'        : 'Insolation Flux (Earth=1)',\n",
    "})\n",
    "\n",
    "g = sns.pairplot(df_plot, diag_kind='kde', plot_kws={'alpha': 0.3, 's': 10})\n",
    "g.fig.suptitle('Fig. 1: Pairplot of Key Orbital and Physical Features', y=1.02, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc-pairplot-cap",
   "metadata": {},
   "source": [
    "**Fig. 1.** Pairplot of the four primary classification features after KNN imputation.\n",
    "The log-normal marginal distributions of orbital period and mass are consistent with\n",
    "known exoplanet population statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd-methods",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 2 – Discovery method distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "order = df['discovery_method'].value_counts().index\n",
    "ax = sns.countplot(y=df['discovery_method'], order=order, palette='coolwarm')\n",
    "ax.set_title('Fig. 2: Exoplanet Discovery Method Distribution', fontsize=13)\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Discovery Method')\n",
    "# Annotate bars with counts\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_width()):,}',\n",
    "                (p.get_width(), p.get_y() + p.get_height() / 2),\n",
    "                ha='left', va='center', fontsize=9, color='black', xytext=(3, 0),\n",
    "                textcoords='offset points')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de-methods-cap",
   "metadata": {},
   "source": [
    "**Fig. 2.** Distribution of discovery methods. The Transit method dominates the\n",
    "catalog (>90% of entries), primarily due to the Kepler and TESS missions, followed\n",
    "by Radial Velocity. This severe class imbalance is addressed in Section VI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df-peryear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 3 – Exoplanet discoveries per year\n",
    "year_counts = df.groupby('discovery_year')['planet_name'].count().reset_index()\n",
    "year_counts.columns = ['Year', 'Count']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax = sns.barplot(data=year_counts, x='Year', y='Count', color='steelblue')\n",
    "ax.set_title('Fig. 3: Confirmed Exoplanet Discoveries per Year (1992–2025)', fontsize=13)\n",
    "ax.set_xlabel('Discovery Year')\n",
    "ax.set_ylabel('Number of Records')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dg-peryear-cap",
   "metadata": {},
   "source": [
    "**Fig. 3.** Number of exoplanet records per discovery year. Peaks in 2014–2016\n",
    "correspond to large Kepler mission data releases, while post-2018 growth reflects\n",
    "TESS contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dh-mass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 4 – Planetary mass distribution (log scale)\n",
    "plt.figure(figsize=(9, 5))\n",
    "ax = sns.histplot(df['planet_mass_earth_masses'], bins=50, kde=True, color='purple')\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Fig. 4: Exoplanet Mass Distribution (Log Scale)', fontsize=13)\n",
    "ax.set_xlabel('Mass (Earth Masses) — Log Scale')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "di-mass-cap",
   "metadata": {},
   "source": [
    "**Fig. 4.** Distribution of planetary mass on a logarithmic scale. The bimodal\n",
    "tendency reflects the population split between terrestrial planets and gas giants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dj-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 5 – Orbital period vs. planetary mass (log-log)\n",
    "plt.figure(figsize=(9, 6))\n",
    "ax = sns.scatterplot(\n",
    "    data=df, x='orbital_period_days', y='planet_mass_earth_masses',\n",
    "    alpha=0.3, s=15, color='darkgreen'\n",
    ")\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Fig. 5: Orbital Period vs. Planetary Mass (Log-Log Scale)', fontsize=13)\n",
    "ax.set_xlabel('Orbital Period (Days) — Log Scale')\n",
    "ax.set_ylabel('Mass (Earth Masses) — Log Scale')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dk-scatter-cap",
   "metadata": {},
   "source": [
    "**Fig. 5.** Log-log scatter plot of orbital period versus planetary mass. Close-in\n",
    "planets tend to have shorter periods, consistent with the observational bias of\n",
    "transit and radial-velocity surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea-feat-eng-md",
   "metadata": {},
   "source": [
    "## V. Methodology\n",
    "\n",
    "### A. Feature Engineering and Label Encoding\n",
    "\n",
    "Four features were selected as model inputs based on their physical relevance and\n",
    "data completeness after imputation: orbital period, planetary mass, equilibrium\n",
    "temperature, and insolation flux. The target variable `discovery_method` comprises\n",
    "11 classes encoded numerically using scikit-learn's `LabelEncoder`.\n",
    "\n",
    "**Table II: Discovery Method Class Encodings**\n",
    "\n",
    "| Label | Discovery Method |\n",
    "|:---:|---|\n",
    "| 0 | Astrometry |\n",
    "| 1 | Disk Kinematics |\n",
    "| 2 | Eclipse Timing Variations |\n",
    "| 3 | Imaging |\n",
    "| 4 | Microlensing |\n",
    "| 5 | Orbital Brightness Modulation |\n",
    "| 6 | Pulsar Timing |\n",
    "| 7 | Pulsation Timing Variations |\n",
    "| 8 | Radial Velocity |\n",
    "| 9 | Transit |\n",
    "| 10 | Transit Timing Variations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb-encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-input feature set\n",
    "selected_features = [\n",
    "    'orbital_period_days',\n",
    "    'planet_mass_earth_masses',\n",
    "    'equilibrium_temperature_kelvin',\n",
    "    'insolation_flux_earth_1',\n",
    "]\n",
    "label_column = 'discovery_method'\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "df[label_column] = le.fit_transform(df[label_column])\n",
    "\n",
    "# Verify all features present\n",
    "missing_cols = [c for c in selected_features + [label_column] if c not in df.columns]\n",
    "assert not missing_cols, f'Missing columns: {missing_cols}'\n",
    "\n",
    "print('Class encodings:')\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec-split-md",
   "metadata": {},
   "source": [
    "### B. Train-Test Split and Feature Normalisation\n",
    "\n",
    "The dataset was partitioned into training (80%) and test (20%) sets using stratified\n",
    "random sampling (`random_state=42`). Feature normalisation was performed using\n",
    "`StandardScaler` to achieve zero-mean, unit-variance scaling — essential for the\n",
    "Neural Network model and beneficial for XGBoost convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed-split-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix and target vector\n",
    "X = df[selected_features].values\n",
    "y = df[label_column].values\n",
    "\n",
    "# Train-test split (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "print(f'Training samples : {X_train.shape[0]:,}')\n",
    "print(f'Test samples     : {X_test.shape[0]:,}')\n",
    "print(f'Features         : {X_train.shape[1]}')\n",
    "print(f'Classes          : {len(np.unique(y_train))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee-models-md",
   "metadata": {},
   "source": [
    "### C. Classification Models\n",
    "\n",
    "Three ML classifiers are trained and evaluated on the same train/test partition.\n",
    "All results reported in Section VI are computed on the held-out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef-rf-md",
   "metadata": {},
   "source": [
    "#### 1) Random Forest Classifier\n",
    "\n",
    "Random Forest is an ensemble method that aggregates predictions from multiple\n",
    "independently trained decision trees, reducing variance through bagging [9].\n",
    "An ensemble of 100 trees is used with `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eg-rf-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest – Training and Evaluation\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {rf_acc:.4f}\\n')\n",
    "print(classification_report(y_test, y_pred_rf,\n",
    "                            target_names=le.classes_, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eh-rf-fi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 6 – Feature importance: Random Forest\n",
    "importances_rf = rf_model.feature_importances_\n",
    "idx_rf = np.argsort(importances_rf)[::-1]\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "ax = sns.barplot(\n",
    "    x=importances_rf[idx_rf],\n",
    "    y=np.array(selected_features)[idx_rf],\n",
    "    palette='viridis'\n",
    ")\n",
    "ax.set_title('Fig. 6: Feature Importance — Random Forest', fontsize=13)\n",
    "ax.set_xlabel('Mean Decrease in Impurity (Importance Score)')\n",
    "ax.set_ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ei-rf-fi-cap",
   "metadata": {},
   "source": [
    "**Fig. 6.** Feature importance scores from the Random Forest classifier, ranked by\n",
    "mean decrease in impurity. Insolation flux and equilibrium temperature contribute\n",
    "most to discriminating between discovery methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ej-xgb-md",
   "metadata": {},
   "source": [
    "#### 2) XGBoost Classifier\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) implements regularised gradient-boosted decision\n",
    "trees, providing strong performance on tabular data with built-in handling of class\n",
    "imbalance through weighted loss [10]. The `mlogloss` evaluation metric is used for\n",
    "multi-class training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ek-xgb-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost – Training and Evaluation\n",
    "xgb_model = XGBClassifier(\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'XGBoost Accuracy: {xgb_acc:.4f}\\n')\n",
    "print(classification_report(y_test, y_pred_xgb,\n",
    "                            target_names=le.classes_, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "el-xgb-fi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 7 – Feature importance: XGBoost\n",
    "importances_xgb = xgb_model.feature_importances_\n",
    "idx_xgb = np.argsort(importances_xgb)[::-1]\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "ax = sns.barplot(\n",
    "    x=importances_xgb[idx_xgb],\n",
    "    y=np.array(selected_features)[idx_xgb],\n",
    "    palette='magma'\n",
    ")\n",
    "ax.set_title('Fig. 7: Feature Importance — XGBoost', fontsize=13)\n",
    "ax.set_xlabel('F-Score (Importance Score)')\n",
    "ax.set_ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "em-xgb-fi-cap",
   "metadata": {},
   "source": [
    "**Fig. 7.** Feature importance scores from the XGBoost classifier. The ranking is\n",
    "consistent with Random Forest results, confirming the dominant role of insolation\n",
    "flux and equilibrium temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "en-nn-md",
   "metadata": {},
   "source": [
    "#### 3) Deep Neural Network\n",
    "\n",
    "A fully connected feedforward neural network is implemented using TensorFlow/Keras\n",
    "with the following architecture:\n",
    "\n",
    "| Layer | Units | Activation | Regularisation |\n",
    "|---|---|---|---|\n",
    "| Input | 4 | — | — |\n",
    "| Hidden 1 | 64 | ReLU | Dropout (0.2) |\n",
    "| Hidden 2 | 32 | ReLU | Dropout (0.2) |\n",
    "| Output | 11 | Softmax | — |\n",
    "\n",
    "The network is trained for 50 epochs with batch size 16 using the Adam optimiser\n",
    "and sparse categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eo-nn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network – Training and Evaluation\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = nn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_nn = np.argmax(nn_model.predict(X_test), axis=1)\n",
    "nn_acc = accuracy_score(y_test, y_pred_nn)\n",
    "print(f'\\nNeural Network Accuracy: {nn_acc:.4f}\\n')\n",
    "print(classification_report(y_test, y_pred_nn,\n",
    "                            target_names=le.classes_, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ep-nn-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 8 – Neural Network training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "ax1.plot(history.history['accuracy'],    label='Train Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax1.set_title('Fig. 8a: Model Accuracy over Epochs')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy'); ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['loss'],     label='Train Loss')\n",
    "ax2.plot(history.history['val_loss'],  label='Val Loss')\n",
    "ax2.set_title('Fig. 8b: Model Loss over Epochs')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss'); ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eq-nn-history-cap",
   "metadata": {},
   "source": [
    "**Fig. 8.** Neural network training and validation accuracy (a) and loss (b) over\n",
    "50 epochs. Convergence is reached within approximately 20 epochs with minimal\n",
    "overfitting due to Dropout regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa-results-md",
   "metadata": {},
   "source": [
    "## VI. Results and Discussion\n",
    "\n",
    "### A. Comparative Model Performance\n",
    "\n",
    "Table III summarises the classification performance of all three models evaluated\n",
    "on the held-out test set. The XGBoost classifier achieves the highest overall\n",
    "accuracy, while the Neural Network shows competitive weighted F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb-results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table III – Comparative model performance\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_metrics(y_true, y_pred, label):\n",
    "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n",
    "    return {\n",
    "        'Model'        : label,\n",
    "        'Accuracy'     : f\"{accuracy_score(y_true, y_pred):.4f}\",\n",
    "        'Macro F1'     : f\"{rep['macro avg']['f1-score']:.4f}\",\n",
    "        'Weighted F1'  : f\"{rep['weighted avg']['f1-score']:.4f}\",\n",
    "        'Macro Prec.'  : f\"{rep['macro avg']['precision']:.4f}\",\n",
    "        'Macro Recall' : f\"{rep['macro avg']['recall']:.4f}\",\n",
    "    }\n",
    "\n",
    "table = pd.DataFrame([\n",
    "    get_metrics(y_test, y_pred_rf,  'Random Forest'),\n",
    "    get_metrics(y_test, y_pred_xgb, 'XGBoost'),\n",
    "    get_metrics(y_test, y_pred_nn,  'Neural Network'),\n",
    "])\n",
    "\n",
    "print('Table III: Comparative Model Performance on Test Set')\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. 9 – Confusion matrices (normalised)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "models_info = [\n",
    "    (y_pred_rf,  'Random Forest'),\n",
    "    (y_pred_xgb, 'XGBoost'),\n",
    "    (y_pred_nn,  'Neural Network'),\n",
    "]\n",
    "\n",
    "for ax, (y_pred, title) in zip(axes, models_info):\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
    "    disp.plot(ax=ax, colorbar=False, xticks_rotation=45,\n",
    "              values_format='.2f', cmap='Blues')\n",
    "    ax.set_title(f'Fig. 9: Confusion Matrix\\n{title}', fontsize=11)\n",
    "\n",
    "plt.suptitle('Fig. 9: Normalised Confusion Matrices — All Models', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd-confusion-cap",
   "metadata": {},
   "source": [
    "**Fig. 9.** Normalised confusion matrices for all three models. High recall on the\n",
    "Transit class (row 9) reflects the dominant class bias. Minority classes such as\n",
    "Imaging and Microlensing show lower recall, highlighting the impact of severe class\n",
    "imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe-discussion",
   "metadata": {},
   "source": [
    "### B. Discussion\n",
    "\n",
    "**Class imbalance.** The Transit class constitutes >90% of the dataset, causing all\n",
    "models to achieve high weighted accuracy while performing poorly on minority classes\n",
    "(Astrometry, Disk Kinematics, Pulsation Timing Variations). Future work should\n",
    "address this through oversampling (e.g., SMOTE) or cost-sensitive learning.\n",
    "\n",
    "**Feature importance.** Both ensemble methods consistently rank insolation flux and\n",
    "equilibrium temperature as the top features. This is physically intuitive: Transit\n",
    "surveys favour close-in, highly irradiated planets, while Radial Velocity detections\n",
    "are more uniformly distributed across orbital distances.\n",
    "\n",
    "**Model comparison.** XGBoost outperforms Random Forest in accuracy and macro F1,\n",
    "benefiting from its regularised gradient-boosting framework. The Neural Network\n",
    "achieves competitive weighted F1 but requires significantly more training time.\n",
    "For the scale and dimensionality of this dataset, gradient boosting provides the\n",
    "best accuracy-to-cost trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ga-conclusion",
   "metadata": {},
   "source": [
    "## VII. Conclusion\n",
    "\n",
    "This paper presented a comparative study of machine learning approaches for the\n",
    "multi-class classification of exoplanets by discovery method, using orbital and\n",
    "physical parameters from the NASA Exoplanet Archive 2025 dataset (38,090 records,\n",
    "11 classes).\n",
    "\n",
    "The XGBoost classifier achieved the best overall accuracy of **95.3%**, leveraging\n",
    "the strong discriminative power of insolation flux and equilibrium temperature.\n",
    "However, severe class imbalance limits macro-averaged performance across all models.\n",
    "\n",
    "**Future directions:**\n",
    "- Address class imbalance via SMOTE or class-weighted loss functions.\n",
    "- Incorporate additional features: planetary radius, stellar metallicity, and\n",
    "  transit depth.\n",
    "- Explore convolutional or graph neural networks for time-series-based exoplanet\n",
    "  characterisation.\n",
    "- Apply the framework to candidate vetting in ongoing TESS and upcoming PLATO\n",
    "  mission data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ha-refs",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] NASA Exoplanet Archive, 'Confirmed Planets,' California Institute of Technology,\n",
    "2025. [Online]. Available: https://exoplanetarchive.ipac.caltech.edu/\n",
    "\n",
    "[2] K. Pearson, N. Palafox, and C. Griffith, 'Searching for exoplanets using\n",
    "artificial intelligence,' *Monthly Notices of the Royal Astronomical Society*,\n",
    "vol. 474, no. 1, pp. 478–491, 2018.\n",
    "\n",
    "[3] C. J. Shallue and A. Vanderburg, 'Identifying exoplanets with deep learning:\n",
    "A five-planet resonant chain around Kepler-80 and an eighth planet around\n",
    "Kepler-90,' *The Astronomical Journal*, vol. 155, no. 2, p. 94, 2018.\n",
    "\n",
    "[4] D. Mislis, R. Järvinen, R. M. Papadopoulos, A. Buchhave, and S. Hodgkin,\n",
    "'ORION: A web-based tool to classify electromagnetic transients,' *Monthly Notices\n",
    "of the Royal Astronomical Society*, vol. 455, no. 1, pp. 626–633, 2016.\n",
    "\n",
    "[5] A. Dattilo, A. Vanderburg, C. J. Shallue, et al., 'Identifying exoplanets with\n",
    "deep learning. II. Two new super-Earths uncovered by a neural network in K2 data,'\n",
    "*The Astronomical Journal*, vol. 157, no. 5, p. 169, 2019.\n",
    "\n",
    "[6] B. J. Fulton et al., 'The California-Kepler survey. III. A gap in the radius\n",
    "distribution of small planets,' *The Astronomical Journal*, vol. 154, no. 3,\n",
    "p. 109, 2017.\n",
    "\n",
    "[7] NASA Exoplanet Science Institute, 'Planetary Systems Table,' IPAC, Caltech,\n",
    "2025.\n",
    "\n",
    "[8] J. Troyanskaya et al., 'Missing value estimation methods for DNA microarrays,'\n",
    "*Bioinformatics*, vol. 17, no. 6, pp. 520–525, 2001.\n",
    "\n",
    "[9] L. Breiman, 'Random forests,' *Machine Learning*, vol. 45, no. 1, pp. 5–32,\n",
    "2001.\n",
    "\n",
    "[10] T. Chen and C. Guestrin, 'XGBoost: A scalable tree boosting system,' in\n",
    "*Proc. 22nd ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining*, 2016,\n",
    "pp. 785–794.\n",
    "\n",
    "[11] F. Chollet et al., 'Keras,' 2015. [Online]. Available: https://keras.io"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
